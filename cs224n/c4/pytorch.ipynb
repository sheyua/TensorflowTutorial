{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "perceived-donor",
   "metadata": {},
   "source": [
    "# CS224N: PyTorch Tutorial (Winter '21)\n",
    "\n",
    "### Author: Dilara Soylu\n",
    "\n",
    "In this notebook, we will have a basic introduction to `PyTorch` and work on a demo network solving X task [TODO write the task here]. Following resources have been used in preparation of this notebook:\n",
    "* Matt's tutorial\n",
    "* Official PyTorch tutorial\n",
    "* GAN Class tutorial [TODO properly cite]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-african",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "[PyTorch](https://pytorch.org/) is a machine learning framework that is used in both academia and industry for various applications. PyTorch started of as a more flexible alternative to [TensorFlow](https://www.tensorflow.org/), which is another popular machine learning framework. At the time of its release, `PyTorch` appealed to the users due to its user friendly nature: as opposed to defining static graphs before performing an operation as in `TensorFlow`, `PyTorch` allowed users to define their operations as they go, which is also the approached integrated by `TensorFlow` in its following releases. Although `TensorFlow` is more widely preferred in the industry, `PyTorch` is often times the preferred machine learning framework for researchers. \n",
    "\n",
    "Now that we have learned enough about the background of `PyTorch`, let's start by importing it into our notebook. To install `PyTorch`, you can follow the instructions here. Alternatively, you can open this notebook using `Google Colab`, which already has `PyTorch` installed in its base kernel. Once you are done with the installation process, run the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879df2d",
   "metadata": {},
   "source": [
    "### Constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tensor from a Python List\n",
    "data = [\n",
    "        [0, 1], \n",
    "        [2, 3],\n",
    "        [4, 5]\n",
    "       ]\n",
    "x_python = torch.tensor(data)\n",
    "\n",
    "# Print the tensor\n",
    "x_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba4096",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_python.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-hearing",
   "metadata": {},
   "source": [
    "PyTorch is open source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b59234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the dtype to create a tensor of particular type\n",
    "x_float = torch.tensor(data, dtype=torch.float)\n",
    "x_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dfc477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using the dtype to create a tensor of particular type\n",
    "x_bool = torch.tensor(data, dtype=torch.bool)\n",
    "x_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce07fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_python.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1897b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor(data).dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f115617",
   "metadata": {},
   "source": [
    "#### From a NumPy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0f972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array(data)\n",
    "x_numpy = torch.tensor(arr)\n",
    "x_numpy[:, 0] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237dcd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a24252",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac72e1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy_container = torch.from_numpy(arr)\n",
    "x_numpy_container[:, 0] = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ccd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54446be",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b7a3d8",
   "metadata": {},
   "source": [
    "#### From a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e689c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a base tensor\n",
    "x = torch.tensor([[1., 2.], [3., 4.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tensor of 0s\n",
    "x_zeros = torch.zeros_like(x)\n",
    "x_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46241842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tensor of 1s\n",
    "x_ones = torch.ones_like(x)\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47daf012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tensor where each element is sampled from a uniform distribution\n",
    "# between 0 and 1\n",
    "x_rand = torch.rand_like(x)\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c50308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tensor where each element is sampled from a normal distribution\n",
    "x_randn = torch.randn_like(x)\n",
    "x_randn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d5569",
   "metadata": {},
   "source": [
    "#### Specifying a Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af57729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 2x3x2 tensor of 0s\n",
    "shape = (4, 2, 2)\n",
    "x_zeros = torch.zeros(shape) # x_zeros = torch.zeros(4, 3, 2) is an alternative\n",
    "x_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7304b6ee",
   "metadata": {},
   "source": [
    "#### With torch.arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a41184",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cd9868",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b14a6",
   "metadata": {},
   "source": [
    "#### data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ed17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
    "x = torch.ones(3, 2, dtype=torch.float64)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deacc1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 3x2 tensor, with 3 rows and 2 columns\n",
    "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c56245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_view = x.reshape(-1, 2)\n",
    "x_view[:, 0] = 90\n",
    "x_view, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_view = x.view(-1, 2)\n",
    "x_view[:, 0] = 90\n",
    "x_view, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bbc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a 5x2 tensor, with 5 rows and 2 columns\n",
    "x = torch.arange(10).reshape(5, 2).unsqueeze(2).unsqueeze(0).unsqueeze(0)\n",
    "x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace19060",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed99bf6",
   "metadata": {},
   "source": [
    "#### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7068e260",
   "metadata": {},
   "source": [
    "#### tensor indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an example tensor\n",
    "x = torch.Tensor([\n",
    "                  [[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]], \n",
    "                  [[9, 10], [11, 12]] \n",
    "                 ])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa28245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 0th element, which is the first row\n",
    "x[0] # Equivalent to x[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a5dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top left element of each element in our tensor\n",
    "x[::2, :, 0].view([-1, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb17b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_reshape = x[::2, :, 0].reshape([-1, ])\n",
    "x_reshape[:] = 200\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9241fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[[0, 0, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb945c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[[1, 2], 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb1b92",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31f790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example tensor\n",
    "x = torch.ones((3,2,2))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b4662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform elementwise addition\n",
    "# Use - for subtraction\n",
    "x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f3317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform elementwise multiplication\n",
    "# Use / for division\n",
    "x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b0c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x3 tensor of 6s\n",
    "a = torch.ones((4,3)) * 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e603fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D tensor of 2s\n",
    "b = torch.ones(3) * 2\n",
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849089f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative to a.matmul(b)\n",
    "# a @ b.T returns the same result since b is 1D tensor and the 2nd dimension\n",
    "# is inferred\n",
    "a @ b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff557a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example tensor\n",
    "m = torch.tensor(\n",
    "    [\n",
    "     [1., 1.],\n",
    "     [2., 2.],\n",
    "     [3., 3.],\n",
    "     [4., 4.]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Mean: {}\".format(m.mean()))\n",
    "print(\"Mean in the 0th dimension: {}\".format(m.mean(0)))\n",
    "print(\"Mean in the 1st dimension: {}\".format(m.mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58756851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate in dimension 0 and 1\n",
    "a_cat0 = torch.cat([a, a, a], dim=0)\n",
    "a_cat1 = torch.cat([a, a, a], dim=1)\n",
    "\n",
    "print(\"Initial shape: {}\".format(a.shape))\n",
    "print(\"Shape after concatenation in dimension 0: {}\".format(a_cat0.shape))\n",
    "print(\"Shape after concatenation in dimension 1: {}\".format(a_cat1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a427d1",
   "metadata": {},
   "source": [
    "### Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b1a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example tensor\n",
    "# requires_grad parameter tells PyTorch to store gradients\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "\n",
    "# Print the gradient if it is calculated\n",
    "# Currently None since x is a scalar\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * x * 3\n",
    "y.backward()\n",
    "z = x * x * 3\n",
    "u = y + z * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672b13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(x, torch.autograd.Variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a51884",
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(a, torch.autograd.Variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576aa602",
   "metadata": {},
   "source": [
    "### Neural Network Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f0c992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e454e1b0",
   "metadata": {},
   "source": [
    "#### Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d7e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the inputs\n",
    "inputs = torch.ones(2,3,4)\n",
    "\n",
    "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
    "# dimensional outputs\n",
    "linear = nn.Linear(4, 2)\n",
    "linear_output = linear(inputs)\n",
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5218b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight, linear.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491149e5",
   "metadata": {},
   "source": [
    "#### Other Module Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece794cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0f9290",
   "metadata": {},
   "source": [
    "#### Nonlinear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(linear_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2810b5c",
   "metadata": {},
   "source": [
    "#### Squential Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad4e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "block = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "inputs = torch.ones(2,3,4)\n",
    "output = block(inputs)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3babccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in block.named_children():\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ca9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "block[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f836a9b1",
   "metadata": {},
   "source": [
    "#### Custom Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb35c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    # Call to the __init__ function of the super class\n",
    "    super(MultilayerPerceptron, self).__init__()\n",
    "\n",
    "    # Bookkeeping: Saving the initialization parameters\n",
    "    self.input_size = input_size \n",
    "    self.hidden_size = hidden_size \n",
    "\n",
    "    # Defining of our layers\n",
    "    self.linear = nn.Linear(self.input_size, self.hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.linear2 = nn.Linear(self.hidden_size, self.input_size)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "  def forward(self, x):\n",
    "    linear = self.linear(x)\n",
    "    relu = self.relu(linear)\n",
    "    linear2 = self.linear2(relu)\n",
    "    output = self.sigmoid(linear2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac35f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a sample input\n",
    "inputs = torch.randn(2, 5)\n",
    "\n",
    "# Create our model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Pass our input through our model\n",
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa926635",
   "metadata": {},
   "outputs": [],
   "source": [
    "output1 = inputs @ model.linear.weight.T + model.linear.bias\n",
    "output2 = model.relu(output1)\n",
    "output3 = model.linear2(output2)\n",
    "output4 = model.sigmoid(output3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in model.named_children():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba3cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ab3ed",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73864cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11197d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the y data\n",
    "y = torch.ones(10, 5)\n",
    "\n",
    "# Add some noise to our goal y to generate our x\n",
    "# We want out model to predict our original data, albeit the noise\n",
    "x = y + torch.randn_like(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf37de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MultilayerPerceptron(5, 3)\n",
    "\n",
    "# Define the optimizer\n",
    "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
    "\n",
    "# Define loss using a predefined loss function\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "# Calculate how our model is doing now\n",
    "y_pred = model(x)\n",
    "loss_function(y_pred, y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4429e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of epoch, which determines the number of training iterations\n",
    "n_epoch = 10 \n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "  # Set the gradients to 0\n",
    "  adam.zero_grad()\n",
    "\n",
    "  # Get the model predictions\n",
    "  y_pred = model(x)\n",
    "\n",
    "  # Get the loss\n",
    "  loss = loss_function(y_pred, y)\n",
    "\n",
    "  # Print stats\n",
    "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
    "\n",
    "  # Compute the gradients\n",
    "  loss.backward()\n",
    "\n",
    "  # Take a step to optimize the weights\n",
    "  adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how our model performs on the training data\n",
    "y_pred = model(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90d661",
   "metadata": {},
   "source": [
    "## Demo: Word Window Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7b011b",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb7877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "    \"We always come to Paris\",\n",
    "    \"The professor is from Australia\",\n",
    "    \"I live in Stanford\",\n",
    "    \"He comes from Taiwan\",\n",
    "    \"The capital of Turkey is Ankara\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca54052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preprocessing function we will use to generate our training examples\n",
    "# Our function is a simple one, we lowercase the letters\n",
    "# and then tokenize the words.\n",
    "def preprocess_sentence(sentence): return sentence.lower().split()\n",
    "\n",
    "# Create our training set\n",
    "train_sentences = [sent.lower().split() for sent in corpus]\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set of locations that appear in our corpus\n",
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "\n",
    "# Our train labels\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c36c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the unique words in our corpus \n",
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8619de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unknown token to our vocabulary\n",
    "vocabulary.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ef953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the <pad> token to our vocabulary\n",
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "# Function that pads the given sentence\n",
    "# We are introducing this function here as an example\n",
    "# We will be utilizing it later in the tutorial\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "  window = [pad_token] * window_size\n",
    "  return window + sentence + window\n",
    "\n",
    "# Show padding example\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are just converting our vocabularly to a list to be able to index into it\n",
    "# Sorting is not necessary, we sort to show an ordered word_to_ind dictionary\n",
    "# That being said, we will see that having the index for the padding token\n",
    "# be 0 is convenient as some PyTorch functions use it as a default value\n",
    "# such as nn.utils.rnn.pad_sequence, which we will cover in a bit\n",
    "ix_to_word = sorted(list(vocabulary))\n",
    "\n",
    "# Creating a dictionary to find the index of a given word\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322635f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a sentence of tokens, return the corresponding indices\n",
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "  indices = []\n",
    "  for token in sentence:\n",
    "    # Check if the token is in our vocabularly. If it is, get it's index. \n",
    "    # If not, get the index for the unknown token.\n",
    "    if token in word_to_ix:\n",
    "      index = word_to_ix[token]\n",
    "    else:\n",
    "      index = word_to_ix[\"<unk>\"]\n",
    "    indices.append(index)\n",
    "  return indices\n",
    "\n",
    "\n",
    "# Show an example\n",
    "example_sentence = [\"we\", \"always\", \"come\", \"to\", \"kuwait\"]\n",
    "example_indices = convert_token_to_indices(example_sentence, word_to_ix)\n",
    "restored_example = [ix_to_word[ind] for ind in example_indices]\n",
    "\n",
    "print(f\"Original sentence is: {example_sentence}\")\n",
    "print(f\"Going from words to indices: {example_indices}\")\n",
    "print(f\"Going from indices to words: {restored_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b300f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our sentences to indices\n",
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1361799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an embedding table for our words\n",
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "# Printing the parameters in our embedding table\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20a2698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the embedding for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a3ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also get multiple embeddings at once\n",
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8d28d",
   "metadata": {},
   "source": [
    "#### Batching Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*zip(train_sentences, train_labels))\n",
    "# Pad the train examples.\n",
    "x = [pad_window(s, window_size=window_size) for s in x]\n",
    "# Convert the train examples into indices.\n",
    "x = [convert_token_to_indices(s, word_to_ix) for s in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d56508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now pad the examples so that the lengths of all the example in \n",
    "# one batch are the same, making it possible to do matrix operations. \n",
    "# We set the batch_first parameter to True so that the returned matrix has \n",
    "# the batch as the first dimension.\n",
    "pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "# pad_sequence function expects the input to be a tensor, so we turn x into one\n",
    "x = [torch.LongTensor(x_i) for x_i in x]\n",
    "x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd64a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also pad the labels. Before doing so, we will record the number \n",
    "# of labels so that we know how many words existed in each example. \n",
    "lengths = [len(label) for label in y]\n",
    "lenghts = torch.LongTensor(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52402d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [torch.LongTensor(y_i) for y_i in y]\n",
    "y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa43a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340486f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def custom_collate_fn(batch, window_size, word_to_ix):\n",
    "    x, y = zip(*batch)\n",
    "    # Pad the train examples.\n",
    "    x = [pad_window(s, window_size=window_size) for s in x]\n",
    "    # Convert the train examples into indices.\n",
    "    x = [convert_token_to_indices(s, word_to_ix) for s in x]\n",
    "    # We will now pad the examples so that the lengths of all the example in \n",
    "    # one batch are the same, making it possible to do matrix operations. \n",
    "    # We set the batch_first parameter to True so that the returned matrix has \n",
    "    # the batch as the first dimension.\n",
    "    pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "    # pad_sequence function expects the input to be a tensor, so we turn x into one\n",
    "    x = [torch.LongTensor(x_i) for x_i in x]\n",
    "    x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "    # We will also pad the labels. Before doing so, we will record the number \n",
    "    # of labels so that we know how many words existed in each example. \n",
    "    lengths = [len(label) for label in y]\n",
    "    lenghts = torch.LongTensor(lengths)\n",
    "    \n",
    "    y = [torch.LongTensor(y_i) for y_i in y]\n",
    "    y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "    # We are now ready to return our variables. The order we return our variables\n",
    "    # here will match the order we read them in our training loop.\n",
    "    return x_padded, y_padded, lenghts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3b3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be passed to the DataLoader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "    print(f\"Iteration {counter}\")\n",
    "    print(\"Batched Input:\")\n",
    "    print(batched_x)\n",
    "    print(\"Batched Labels:\")\n",
    "    print(batched_y)\n",
    "    print(\"Batched Lengths:\")\n",
    "    print(batched_lengths)\n",
    "    print(\"\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d78166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original tensor\n",
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "# Create the 2 * 2 + 1 chunks\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7584c026",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb1aa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "        super(WordWindowClassifier, self).__init__()\n",
    "\n",
    "        \"\"\" Instance variables \"\"\"\n",
    "        self.window_size = hyperparameters[\"window_size\"]\n",
    "        self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "        self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "        self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "        \"\"\" Embedding Layer \n",
    "        Takes in a tensor containing embedding indices, and returns the \n",
    "        corresponding embeddings. The output is of dim \n",
    "        (number_of_indices * embedding_dim).\n",
    "\n",
    "        If freeze_embeddings is True, set the embedding layer parameters to be\n",
    "        non-trainable. This is useful if we only want the parameters other than the\n",
    "        embeddings parameters to change. \n",
    "\n",
    "        \"\"\"\n",
    "        self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "        \"\"\" Hidden Layer\n",
    "        \"\"\"\n",
    "        full_window_size = 2 * window_size + 1\n",
    "        self.hidden_layer = nn.Sequential(\n",
    "            nn.Linear(full_window_size * self.embed_dim, self.hidden_dim), \n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        \"\"\" Output Layer\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "        \"\"\" Probabilities \n",
    "        \"\"\"\n",
    "        self.probabilities = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "\n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L) LongTensor\n",
    "        Outputs a (B, L~, S) LongTensor\n",
    "        \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding.\n",
    "        Takes in a torch.LongTensor of size (B, L~, S) \n",
    "        Outputs a (B, L~, S, D) FloatTensor.\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L~, S, D) FloatTensor.\n",
    "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 1.\n",
    "        Takes in a (B, L~, S*D) FloatTensor.\n",
    "        Resizes it into a (B, L~, H) FloatTensor\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 2\n",
    "        Takes in a (B, L~, H) FloatTensor.\n",
    "        Resizes it into a (B, L~, 1) FloatTensor.\n",
    "        \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        \"\"\"\n",
    "        Softmax.\n",
    "        Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "        Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
    "        \"\"\"\n",
    "        output = self.probabilities(output)\n",
    "        output = output.view(B, -1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f993136",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d79fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 10,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db591476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):   \n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the \n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07174e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "\n",
    "    # Keep track of the total loss for the batch\n",
    "    total_loss = 0\n",
    "    for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Run a forward pass\n",
    "        outputs = model.forward(batch_inputs)\n",
    "        # Compute the batch loss\n",
    "        loss = loss_function(outputs, batch_labels, batch_lengths)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the parameteres\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Function containing our main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "\n",
    "    # Iterate through each epoch and call our train_epoch function\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "        if epoch % 100 == 0: print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bba5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ad410",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([\n",
    "    [ 0,  0, 22,  2,  6, 20, 15,  0,  0,  0],\n",
    "    [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a3c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, L = inputs.size()\n",
    "token_windows = inputs.unfold(1, 2 * model.window_size + 1, 1)\n",
    "assert token_windows.size() == (B, L - 2 * model.window_size, 2 * model.window_size + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f85d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_windows = model.embeds(token_windows)\n",
    "embedded_windows = embedded_windows.view(B, L - 2 * model.window_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e48590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_1 = model.hidden_layer(embedded_windows)\n",
    "output = model.output_layer(layer_1)\n",
    "output = model.probabilities(output)\n",
    "output = output.view(B, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b999740",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0b13c7",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7dfcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test sentences\n",
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "# Create a test loader\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = False\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                           batch_size=1, \n",
    "                                           shuffle=False, \n",
    "                                           collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecbf9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "    outputs = model.forward(test_instance)\n",
    "    print(labels)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a30a2eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
